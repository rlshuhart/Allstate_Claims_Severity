{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "import matplotlib as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import training data set\n",
    "train = pd.read_csv(\"../data/raw/train.csv.zip\", compression=\"zip\", index_col='id')\n",
    "meta_data = pd.read_csv(\"../data/interim/training_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Names\n",
    "cat_names = meta_data[meta_data['Data Type']=='object']['Variable']\n",
    "cont_names = meta_data[(meta_data['Data Type']=='float64') & \n",
    "                       (meta_data['Variable']!='loss')]['Variable']\n",
    "\n",
    "# Merge transformed categorical data with continuous\n",
    "cont_names_to_drop =['cont12' # highly correlated to cont11\n",
    "                     ,'cont9' # highly correlated to cont1\n",
    "                    ]\n",
    "cont_names_final = cont_names[~cont_names.isin(cont_names_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "encoder = ce.BinaryEncoder()\n",
    "\n",
    "categorical_data = encoder.fit_transform(train[cat_names])\n",
    "continuous_data = train[cont_names]\n",
    "\n",
    "train_final1 = pd.concat([train[cont_names], categorical_data], axis=1)\n",
    "train_final1.to_csv(\"../data/interim/train_binary_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "clf = linear_model.SGDRegressor()\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring=scorer)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [{'loss':['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "               'penalty':['none', 'l2', 'l1', 'elasticnet']\n",
    "              }]\n",
    "\n",
    "gs = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring=scorer)\n",
    "gs = gs.fit(X, y)\n",
    "\n",
    "print(\"Best F-Score: \", gs.best_score_)\n",
    "print(\"Best Parameters: \", gs.best_params_)\n",
    "print(\"Best Estimator: \", gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_final1 = pd.read_csv(\"../data/interim/train_binary_encoded.csv\")\n",
    "X=train_final1#.values\n",
    "y=train['loss']#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.1, test_size=0.1)\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=3, scoring='mean_absolute_error')\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('../models/tpot_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
