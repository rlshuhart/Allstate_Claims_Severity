{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 9.4 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train = pd.read_csv(\"../data/raw/train.csv.zip\", compression=\"zip\", usecols=['loss'])\n",
    "train_binary = pd.read_pickle(\"../data/processed/train_binary_encoded.p\")\n",
    "\n",
    "X=train_binary\n",
    "y=train['loss']\n",
    "\n",
    "# Chosing a lesser training size of 60% to avoid over fitting.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.6, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based Pipeline Optimization Tool  (TPOT)\n",
    "Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). Automating biomedical data science through tree-based pipeline optimization. Applications of Evolutionary Computation, pages 123-137.\n",
    "\n",
    "http://rhiever.github.io/tpot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def go_tpot():\n",
    "    import pandas as pd\n",
    "    from tpot import TPOTRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    tpot = TPOTRegressor(generations=5, population_size=20, verbosity=3, scoring='mean_absolute_error')\n",
    "    tpot.fit(X_train, y_train)\n",
    "    print(tpot.score(X_test, y_test))\n",
    "    tpot.export('../models/tpot_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone - SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def go_sgd():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "    scorer = make_scorer(mean_absolute_error)\n",
    "    clf = linear_model.SGDRegressor()\n",
    "\n",
    "    # scores = cross_val_score(clf, X, y, cv=10, scoring=scorer)\n",
    "    # print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    param_grid = [{'loss':['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "                   'penalty':['none', 'l2', 'l1', 'elasticnet']\n",
    "                  }]\n",
    "\n",
    "    gs = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3, n_jobs=-1, scoring=scorer)\n",
    "    gs = gs.fit(X, y)\n",
    "\n",
    "    print(\"Best F-Score: \", gs.best_score_)\n",
    "    print(\"Best Parameters: \", gs.best_params_)\n",
    "    print(\"Best Estimator: \", gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
